# -*- coding: utf-8 -*-
"""TaskWhiz_Bot.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1CVy7SVPOdCmIT8FoHQyhoCf_VqAaaqc9
"""

!pip install langchain transformers sentence-transformers chromadb

import warnings
warnings.filterwarnings("ignore", category=DeprecationWarning)
warnings.filterwarnings("ignore", category=FutureWarning)
warnings.filterwarnings("ignore")  # catches any other general warning

!pip install -U langchain-community

from langchain.embeddings import HuggingFaceEmbeddings

embedding = HuggingFaceEmbeddings(model_name="all-MiniLM-L6-v2")

import zipfile
import os

# Define the path to the zip file and extraction directory
zip_path = "/content/Knowledge_Base.zip"
extract_dir = "Knowledge_Base"

# Extract contents
with zipfile.ZipFile(zip_path, 'r') as zip_ref:
    zip_ref.extractall(extract_dir)

print(f"Extracted files to {extract_dir}")

!pip install unstructured

from langchain_community.document_loaders import DirectoryLoader
from langchain.text_splitter import RecursiveCharacterTextSplitter

# Load all .md or .txt files from extracted folder
loader = DirectoryLoader("Knowledge_Base/Knowledge_Base", glob="**/*")
docs = loader.load()

# Split into chunks
splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=100)
chunks = splitter.split_documents(docs)

from langchain_community.embeddings import HuggingFaceEmbeddings
from langchain_community.vectorstores import Chroma

# Initialize embeddings model
embedding = HuggingFaceEmbeddings(model_name="all-MiniLM-L6-v2")

# Create and persist Chroma vector store
vectorstore = Chroma.from_documents(
    documents=chunks,
    embedding=embedding,
    persist_directory="./chroma_db"
)

vectorstore.persist()

# Create retriever for querying
retriever = vectorstore.as_retriever(search_kwargs={"k": 3})

!pip install cohere

!pip install --upgrade langchain

# !pip install -U langchain langchain-community

from langchain.retrievers import ContextualCompressionRetriever
from langchain.retrievers.document_compressors import CohereRerank

compressor = CohereRerank(
    model="rerank-english-v2.0",
    cohere_api_key="0YxoSvnBL9xhWIfmgO4WO1hmu3gn64TF6pSvBtjG"
)  # Requires Cohere API key in environment
rerank_retriever = ContextualCompressionRetriever(
    base_compressor=compressor,
    base_retriever=retriever
)

from langchain.llms import HuggingFacePipeline
from transformers import pipeline

# Set up Hugging Face inference pipeline
qa_pipeline = pipeline("text2text-generation", model="google/flan-t5-base", max_length=512, temperature=0.7, top_k=50)

# Wrap in LangChain
llm = HuggingFacePipeline(pipeline=qa_pipeline)

from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, pipeline
from langchain_community.llms import HuggingFacePipeline
from langchain.chains import ConversationalRetrievalChain
from langchain.memory import ConversationBufferMemory
from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, pipeline
from langchain_community.llms import HuggingFacePipeline
from langchain.chains import LLMChain
from langchain.prompts import PromptTemplate  # ðŸ‘ˆ THIS LINE
from langchain.schema.runnable import RunnableLambda
from langchain.memory import ConversationBufferWindowMemory


# Load Hugging Face model and tokenizer
model_id = "google/flan-t5-base"
tokenizer = AutoTokenizer.from_pretrained(model_id)
model = AutoModelForSeq2SeqLM.from_pretrained(model_id)

# Create HF pipeline
hf_pipeline = pipeline("text2text-generation", model="google/flan-t5-base", max_length=512, temperature=0.7, top_k=50)
llm = HuggingFacePipeline(pipeline=hf_pipeline)

memory = ConversationBufferWindowMemory(
    memory_key="chat_history",
    return_messages=True,
    k=3  # Only the last 3 exchanges are kept in memory
)

def format_chat_history(chat_history):
  return "\n".join([f"User: {m.content}" if m.type == "human" else f"Bot: {m.content}" for m in chat_history])


custom_prompt = PromptTemplate.from_template("""
You are an intelligent and helpful AI assistant for a SaaS product called **TaskWhiz**.

Your job is to answer user questions **clearly, accurately, and helpfully**, using:
- The **retrieved context**
- The **recent conversation history**

Rules:
- If the question is a follow-up, use past history to respond accurately.
- Never repeat the same advice multiple times.
- If unsure or context is lacking, respond: "I'm not sure based on the available information."
- Avoid overly short answers like "Yes." or "No." â€” provide helpful reasoning or links.
- Keep responses concise, but informative.
- Highlight key actions in **bold** when guiding users.

---

Chat History:
{chat_history}

Relevant Context:
{context}

Current Question:
{question}

Answer:
""")


qa_chain = ConversationalRetrievalChain.from_llm(
    llm=llm,
    retriever=rerank_retriever,  # your CohereRerank wrapped retriever
    memory=memory,
    return_source_documents=False,
    combine_docs_chain_kwargs={"prompt": custom_prompt},
    output_key="answer"
)

from transformers import logging
warnings.filterwarnings("ignore", category=UserWarning)
logging.set_verbosity_error()

import warnings
from sklearn.feature_extraction.text import TfidfVectorizer
from collections import defaultdict
import re
from langchain_core._api.deprecation import LangChainDeprecationWarning
warnings.filterwarnings("ignore", category=LangChainDeprecationWarning)

from collections import defaultdict

from collections import defaultdict

print("ðŸ¤– Ask your SaaS product questions! (type 'exit' to quit)")

fallback_count = 0
faq_hit_count = 0
question_keywords = defaultdict(int)
total_questions = 0

while True:
    query = input("\nYou: ")

    if query.lower() in ["exit", "quit"]:
        print("\nðŸ“Š Session Summary:")
        print(f"Total questions: {total_questions}")
        print(f"FAQ answers: {faq_hit_count}")
        print(f"Fallbacks triggered: {fallback_count}")
        print("Top keyword categories:")
        for kw, count in question_keywords.items():
            print(f" - {kw}: {count}")
        break

    total_questions += 1

    # --- Track keyword categories ---
    keywords = ["pricing", "billing", "account", "setup", "integration", "export", "report", "automation"]
    for word in keywords:
        if word in query.lower():
            question_keywords[word] += 1

    # --- Chat history formatting ---
    chat_history = memory.chat_memory.messages
    formatted_history = format_chat_history(chat_history)

    # --- Bot response sanitization ---
    def sanitize_bot_response(response):
        trimmed = response.strip().lower()

        if trimmed in ["yes", "no"]:
            return "I'm not sure based on the available information. Could you clarify?"
        if trimmed in ["backups", "taskwhiz"]:
            return "Could you clarify what you need related to backups or TaskWhiz?"
        if "they compare with" in response:
            return "Could you clarify what features you're comparing? Thatâ€™ll help me give a better answer."
        if "Make sure your card is valid and not expired" in response:
            return response + " If you're still having issues, please contact support via live chat or email."

        response = re.sub(r"[*_`]", "", response)

        return response

    # --- Invoke the bot ---
    result = qa_chain.invoke({
        "question": query,
        "chat_history": formatted_history
    })

    raw_answer = result.get("answer", "").strip()
    sanitized_answer = sanitize_bot_response(raw_answer)

    # --- Fallback check ---
    if not sanitized_answer or "I'm not sure" in sanitized_answer:
        fallback_count += 1
    else:
        faq_hit_count += 1

    print("\nBot:", sanitized_answer)

    # --- Optional: feedback tracking ---
    feedback = input("ðŸ¤” Was this answer helpful? (y/n): ").strip().lower()
    if feedback == "n":
        print("ðŸ“Œ Got it. Iâ€™ll use that to improve future responses.")

